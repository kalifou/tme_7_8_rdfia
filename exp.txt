##########################################################################################
######################### AFTER BATCH-NORMALIZING #########################################

Influence of Learning Rate lr:
See : screenshots

===>best lr = 0.01

##########################################################################################

Influence of Batch size w/ lr 0.01
(moyenne de précision lors de la dernière epoch = 5)

batch-size : 216
@epoch:5, Total time 76s	Avg loss 1.3988	Avg Prec@1 49.82 %	Avg Prec@5 92.78 %

batch-size : 128
@epoch:5, Total time 100s	Avg loss 1.1356	Avg Prec@1 59.90 %	Avg Prec@5 95.48 %

batch-size : 64
@epoch:5, Total time 217s	Avg loss 0.9357	Avg Prec@1 67.05 %	Avg Prec@5 97.04 %

batch-size : 32

@epoch:5, Total time 461s	Avg loss 0.8417	Avg Prec@1 70.60 %	Avg Prec@5 97.75 %

batch-size : 16
....pretty slow but the accuracy keeps increasing.
The best performances might be with a batch-size of 1, ie Stochastic Gradient Descent !!


############################################################################################
############################################################################################
############################################################################################
############################################################################################
######################### AFTER BATCH-NORMALIZING #########################################
### python3 base.py --lr 0.01 --cifar --cuda

=================
=== EPOCH 1 =====
=================

[TRAIN Batch 000/391]	Time 0.915s (0.915s)	Loss 2.3038 (2.3038)	Prec@1   5.5 (  5.5)	Prec@5  50.0 ( 50.0)
/opt/anaconda3/lib/python3.6/site-packages/matplotlib/backend_bases.py:2453: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented
  warnings.warn(str, mplDeprecation)
  [TRAIN Batch 050/391]	Time 0.035s (0.138s)	Loss 2.2961 (2.3001)	Prec@1  17.2 (  9.8)	Prec@5  52.3 ( 52.0)
  [TRAIN Batch 100/391]	Time 0.034s (0.452s)	Loss 2.2886 (2.2954)	Prec@1   7.8 ( 11.4)	Prec@5  62.5 ( 55.7)
  [TRAIN Batch 150/391]	Time 0.034s (0.615s)	Loss 2.2496 (2.2882)	Prec@1  19.5 ( 12.2)	Prec@5  64.1 ( 58.0)
  [TRAIN Batch 200/391]	Time 0.035s (0.655s)	Loss 2.2064 (2.2748)	Prec@1  25.0 ( 14.0)	Prec@5  60.9 ( 60.4)
  [TRAIN Batch 250/391]	Time 0.034s (0.969s)	Loss 2.1429 (2.2488)	Prec@1  21.1 ( 15.6)	Prec@5  68.0 ( 62.8)
  [TRAIN Batch 300/391]	Time 0.034s (1.445s)	Loss 1.8922 (2.2125)	Prec@1  28.9 ( 17.5)	Prec@5  79.7 ( 65.3)
  [TRAIN Batch 350/391]	Time 0.035s (1.422s)	Loss 2.0365 (2.1768)	Prec@1  18.8 ( 19.1)	Prec@5  73.4 ( 67.4)

===============> Total time 589s    Avg loss 2.1493  Avg Prec@1 20.17 %	Avg Prec@5 68.81 %

[EVAL Batch 000/079]   Time 0.044s (0.044s)  Loss 1.8442 (1.8442)     Prec@1  35.9 ( 35.9)	Prec@5  82.0 ( 82.0)
[EVAL Batch 050/079]   Time 0.010s (0.011s)  Loss 1.8507 (1.8839)     Prec@1  32.0 ( 32.1)	Prec@5  82.0 ( 82.0)

===============> Total time 0s	   Avg loss 1.8772	 Avg Prec@1 31.80 %   Avg Prec@5 82.19 %

=================
=== EPOCH 2 =====
=================

[TRAIN Batch 000/391]	Time 0.055s (0.055s)	Loss 1.9221 (1.9221)	Prec@1  30.5 ( 30.5)	Prec@5  80.5 ( 80.5)
[TRAIN Batch 050/391]	Time 0.035s (1.047s)	Loss 1.7911 (1.8699)	Prec@1  31.2 ( 31.5)	Prec@5  87.5 ( 83.1)
[TRAIN Batch 100/391]	Time 0.034s (1.856s)	Loss 1.7977 (1.8516)	Prec@1  30.5 ( 32.2)	Prec@5  83.6 ( 83.9)


[TRAIN Batch 150/391]	Time 0.034s (1.473s)	Loss 1.8020 (1.8336)	Prec@1  40.6 ( 32.8)	Prec@5  81.2 ( 84.7)



[TRAIN Batch 200/391]	Time 0.034s (1.621s)	Loss 1.8356 (1.8145)	Prec@1  35.2 ( 33.6)	Prec@5  79.7 ( 85.3)
[TRAIN Batch 250/391]	Time 0.035s (1.831s)	Loss 1.7253 (1.7959)	Prec@1  35.2 ( 34.2)	Prec@5  89.1 ( 85.7)
[TRAIN Batch 300/391]	Time 0.034s (1.581s)	Loss 1.6002 (1.7770)	Prec@1  41.4 ( 34.9)	Prec@5  92.2 ( 86.1)
[TRAIN Batch 350/391]	Time 0.035s (1.513s)	Loss 1.7384 (1.7649)	Prec@1  33.6 ( 35.4)	Prec@5  88.3 ( 86.4)

===============> Total time 534s    Avg loss 1.7518  Avg Prec@1 35.91 %	Avg Prec@5 86.73 %

[EVAL Batch 000/079]   Time 0.046s (0.046s)  Loss 1.6980 (1.6980)     Prec@1  40.6 ( 40.6)	Prec@5  84.4 ( 84.4)
[EVAL Batch 050/079]   Time 0.010s (0.011s)  Loss 1.5846 (1.6648)     Prec@1  42.2 ( 38.7)	Prec@5  90.6 ( 88.3)

===============> Total time 0s	   Avg loss 1.6631	 Avg Prec@1 38.44 %   Avg Prec@5 88.45 %

=================
=== EPOCH 3 =====
=================

[TRAIN Batch 000/391]	Time 0.052s (0.052s)	Loss 1.6508 (1.6508)	Prec@1  39.8 ( 39.8)	Prec@5  89.8 ( 89.8)
[TRAIN Batch 050/391]	Time 0.034s (0.165s)	Loss 1.6437 (1.6182)	Prec@1  43.0 ( 41.3)	Prec@5  89.8 ( 89.4)
[TRAIN Batch 100/391]	Time 0.034s (0.175s)	Loss 1.5972 (1.6111)	Prec@1  36.7 ( 41.2)	Prec@5  89.8 ( 89.6)
[TRAIN Batch 150/391]	Time 0.034s (0.159s)	Loss 1.5500 (1.5984)	Prec@1  43.8 ( 41.7)	Prec@5  92.2 ( 90.1)
[TRAIN Batch 200/391]	Time 0.034s (0.213s)	Loss 1.6774 (1.5923)	Prec@1  39.8 ( 41.9)	Prec@5  85.9 ( 90.3)




[TRAIN Batch 250/391]	Time 0.035s (0.293s)	Loss 1.5219 (1.5833)	Prec@1  42.2 ( 42.2)	Prec@5  91.4 ( 90.6)
[TRAIN Batch 300/391]	Time 0.035s (0.329s)	Loss 1.5374 (1.5755)	Prec@1  39.1 ( 42.4)	Prec@5  93.0 ( 90.6)
[TRAIN Batch 350/391]	Time 0.035s (0.304s)	Loss 1.4950 (1.5694)	Prec@1  52.3 ( 42.8)	Prec@5  89.1 ( 90.7)

===============> Total time 109s    Avg loss 1.5656  Avg Prec@1 42.99 %	Avg Prec@5 90.69 %

[EVAL Batch 000/079]   Time 0.046s (0.046s)  Loss 1.4419 (1.4419)     Prec@1  49.2 ( 49.2)	Prec@5  90.6 ( 90.6)
[EVAL Batch 050/079]   Time 0.010s (0.011s)  Loss 1.4274 (1.4804)     Prec@1  46.1 ( 46.0)	Prec@5  93.8 ( 92.0)

===============> Total time 0s	   Avg loss 1.4863	 Avg Prec@1 45.57 %   Avg Prec@5 92.07 %

=================
=== EPOCH 4 =====
=================

[TRAIN Batch 000/391]	Time 0.057s (0.057s)	Loss 1.5418 (1.5418)	Prec@1  46.9 ( 46.9)	Prec@5  94.5 ( 94.5)
[TRAIN Batch 050/391]	Time 0.035s (0.163s)	Loss 1.6981 (1.5144)	Prec@1  37.5 ( 45.5)	Prec@5  87.5 ( 91.6)
[TRAIN Batch 100/391]	Time 0.034s (0.528s)	Loss 1.4541 (1.4890)	Prec@1  50.0 ( 46.4)	Prec@5  89.8 ( 92.0)

[TRAIN Batch 150/391]	Time 0.035s (0.564s)	Loss 1.5512 (1.4795)	Prec@1  40.6 ( 46.6)	Prec@5  90.6 ( 92.1)
[TRAIN Batch 200/391]	Time 0.034s (0.572s)	Loss 1.5389 (1.4744)	Prec@1  45.3 ( 46.8)	Prec@5  87.5 ( 92.1)
[TRAIN Batch 250/391]	Time 0.034s (0.579s)	Loss 1.3864 (1.4661)	Prec@1  45.3 ( 47.1)	Prec@5  96.1 ( 92.1)
[TRAIN Batch 300/391]	Time 0.035s (0.527s)	Loss 1.4307 (1.4593)	Prec@1  51.6 ( 47.2)	Prec@5  93.0 ( 92.3)
[TRAIN Batch 350/391]	Time 0.034s (0.493s)	Loss 1.4425 (1.4512)	Prec@1  49.2 ( 47.4)	Prec@5  95.3 ( 92.4)

===============> Total time 175s    Avg loss 1.4479  Avg Prec@1 47.57 %	Avg Prec@5 92.44 %

[EVAL Batch 000/079]   Time 0.042s (0.042s)  Loss 1.3540 (1.3540)     Prec@1  52.3 ( 52.3)	Prec@5  93.0 ( 93.0)
[EVAL Batch 050/079]   Time 0.010s (0.011s)  Loss 1.3975 (1.3883)     Prec@1  46.1 ( 50.2)	Prec@5  94.5 ( 93.6)

===============> Total time 0s	   Avg loss 1.3950	 Avg Prec@1 49.81 %   Avg Prec@5 93.37 %

=================
=== EPOCH 5 =====
=================

[TRAIN Batch 000/391]	Time 0.058s (0.058s)	Loss 1.4766 (1.4766)	Prec@1  43.0 ( 43.0)	Prec@5  94.5 ( 94.5)
[TRAIN Batch 050/391]	Time 0.035s (0.105s)	Loss 1.5168 (1.3802)	Prec@1  53.9 ( 51.0)	Prec@5  90.6 ( 93.5)
[TRAIN Batch 100/391]	Time 0.034s (0.110s)	Loss 1.2658 (1.3859)	Prec@1  62.5 ( 51.1)	Prec@5  92.2 ( 93.4)
[TRAIN Batch 150/391]	Time 0.034s (0.136s)	Loss 1.3219 (1.3686)	Prec@1  53.1 ( 51.4)	Prec@5  89.8 ( 93.5)
[TRAIN Batch 200/391]	Time 0.035s (0.117s)	Loss 1.3946 (1.3694)	Prec@1  51.6 ( 51.3)	Prec@5  90.6 ( 93.5)
[TRAIN Batch 250/391]	Time 0.034s (0.126s)	Loss 1.3963 (1.3610)	Prec@1  50.8 ( 51.5)	Prec@5  92.2 ( 93.6)
[TRAIN Batch 300/391]	Time 0.035s (0.125s)	Loss 1.4212 (1.3583)	Prec@1  50.0 ( 51.5)	Prec@5  93.8 ( 93.6)
[TRAIN Batch 350/391]	Time 0.035s (0.116s)	Loss 1.1428 (1.3536)	Prec@1  59.4 ( 51.6)	Prec@5  95.3 ( 93.7)

===============> Total time 50s	    Avg loss 1.3503  Avg Prec@1 51.66 %	Avg Prec@5 93.68 %

[EVAL Batch 000/079]   Time 0.056s (0.056s)  Loss 1.2624 (1.2624)     Prec@1  59.4 ( 59.4)	Prec@5  92.2 ( 92.2)
[EVAL Batch 050/079]   Time 0.010s (0.011s)  Loss 1.3148 (1.2865)     Prec@1  50.0 ( 54.8)	Prec@5  93.8 ( 94.4)

===============> Total time 0s	   Avg loss 1.2958	 Avg Prec@1 54.24 %   Avg Prec@5 94.37 %
